# Classification
Classification refers to the task of giving a machine learning algorithm features, and having the algorithm put the instances/data points into one of many discrete classes. Classes are categorical in nature, it isn't possible for an instance to be classified as partially one class and partially another.
# Command
![](image./main_command.png)
![](image./classifier_command.png)
## FIT example : sample + fit on iris data set with 9 classifications algrs
	| inputlookup iris.csv
	 
	| sample partitions=100 seed=1234
	| search partition_number <= 70

	| fit BernoulliNB species from * into Ber_iris
	| fit DecisionTreeClassifier species from * into DT_iris
	| fit GaussianNB species from * into Gau_iris
	| fit GradientBoostingClassifier species from * into GB_iris
	| fit LogisticRegression species from * into LR_iris
	| fit MLPClassifier species from * into MLP_iris
	| fit RandomForestClassifier species from * into RF_iris
	| fit SGDClassifier species from * into SGD_iris
	| fit SVM species from *  probabilities=True into SVM_iris
## Apply example : sample + apply +score
	| inputlookup iris.csv
	 
	| sample partitions=100 seed=1234
	| search partition_number > 70
	 
	| apply Ber_iris as Ber_species 
	| apply DT_iris as DT_species
	| apply Gau_iris as Gau_species
	| apply GB_iris as GB_species
	| apply LR_iris as LR_species
	| apply MLP_iris as MLP_species
	| apply RF_iris as RF_species
	| apply SGD_iris as SGD_species
	| apply SVM_ris as SVM_species
	| score precision_recall_fscore_support species ~ Ber_species DT_species Gau_species
	GB_species LR_species MLP_species RF_species SGD_species SVM_species average=weighted
	
![](image./classifier.png)
## BernoulliNB : implementation of the Naive Bayes classification algorithms for data that is distributed according to multivariate Bernoulli distributions.
What is Burnoulli (Bern) distributions? 

any event with 1 trial, 2 possible outcomes. Example: coin flip, 1true/1false question, vote for democratic/Republican

What is Multivariate Bern distributions? 

There may be multiple features but each one is assumed to be a binary-valued boolean variable.
Therefore, this class requires samples to be represented as binary-valued feature vectors. If handed any other kind of data, a Bern instance may binarize its input(depending on the binarize parameter)
![](image./BernNB1.png)
![](image./BernNB2.png)

### Example : Diabetes dataset with target var = response has binary outcome(1 or 0)
	| inputlookup diabetes.csv
	| sample partitions=700 seed=1234
	| search partition_number <= 500
	| fit BernoulliNB response from * into BernNB_diabetes_default alpha=1 binarize=0 
	fit_prior=True partial_fit=true
	| fit BernoulliNB response from * into BernNB_diabetes alpha=0.5 binarize=0 fit_prior=f


	| inputlookup diabetes.csv
	| sample partitions=700 seed=1234
	| search partition_number > 500
	| apply BernNB_diabetes_default as pred_res_default
	| apply BernNB_diabetes as pred_response
	| score accuracy_score response ~ pred_res_default pred_response 

![](image./BernNB_diabetes.png)
## DecisionTreeClassifier: capable of performing multi-class classification on a dataset
Decisions tree learn from data to approximate a sine curve with a set of if-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model
![](image./DT2.png)
![](image./DT1.png)

Parameters:

1. max_depth: None(default), keep expanding until all leaves are pure

2. max_feature: nb of features to consider when looking for the best split

![](image./DT_max_feature.png)

3. min_samples_split: int or float, default=1. The minimum number of samples required to be at a leaf node.

4. max_leaf_nodes: int, default=None. Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.

5. criterion: gini(default) = gini impurity / entropy = information gain

6. splitter: best(default)= best split at each node/ random= random split

7. random_state: generate the seed used by the random number generator.

### Example :iris dataset with 3 classes in target var(species)
	| inputlookup iris.csv 
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit DecisionTreeClassifier species from * into DT_iris_default max_features=auto 
	criterion=gini splitter=best random_state=1234
	| fit DecisionTreeClassifier species from * into DT_iris max_features=2 random_state=1234

	| inputlookup iris.csv
	| sample partitions=100 seed=1234
	| search partition_number > 70
	| apply DT_iris_default as DT_species_default
	| apply DT_iris as DT_species
	| score accuracy_score species ~ DT_species_default DT_species

![](image./DT_iris.png)
|summary DT_iris_default

![](image./DT_iris_summary.png)


# Evaluate classifier results
Scoring methods name:

	1. Accuracy scoring
	2. Confusion matrix
	3. F1-score
	4. Precision
	5. Precision-Recall-F1-Support
	6. Recall
	7. ROC-AUC-score
	8. ROC-curve

Syntax:| score <scoring-method-name> <actual_field_1> ... <actual_field_n> against <predicted_field_1> ... <predicted_field_n> [options]
Options: averge = choose from table below
![](image./scoring_average.png)
Look at [Scoring your model](https://docs.splunk.com/Documentation/MLApp/4.2.0/User/ScoreCommand)

## 1. Accuracy scoring : get the prediction accuracy between actual-labels and predicted-labels
![](image./accuracy_score_def.png)

![](image./accuracy_score_ex.png)

Syntax: ...|score accuracy_score <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> normalize=<True|False>

...| score accuracy_score species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species
![](image./classifier_accuracy_score.png)

Result: RF and Decision tree has a highest accuracy_score in this experiment.

## 2. Consusion_matric: a matrix as output and describes the complete performance of the model.
![](image./consusion_matrix_def.png)
	
	True Positives : The cases in which we predicted YES and the actual output was also YES.
	True Negatives : The cases in which we predicted NO and the actual output was NO.
	False Positives : The cases in which we predicted YES and the actual output was NO.
	False Negatives : The cases in which we predicted NO and the actual output was YES.

Syntax: score confusion_matrix <\actual_field> against <\predicted_field>

...| score confusion_matrix species against DT_species

![](image./consusion_matrix_ex.png)

| n= 40                    | Actual  (Iris Setosa) | Actual (Iris Versicolor) | Actual (Iris Virginica) |    |                                                 |
|--------------------------|-----------------------|--------------------------|-------------------------|----|-------------------------------------------------|
| Predict(Iris Setosa)     | 15 (TP)               | 0                        | 0                       | 15 | Accuracy_score = TP+TN/n = (15+9+14)/40 = 0.95 |
| Predict(Iris Versicolor) | 0                     | 9 (TP)                   | 2(Evir/ver)FN           | 11 |                                                 |
| Predict(Iris Virginica)  | 0                     | 0                        | 14 (TP)                 | 14 |                                                 |
|                          | 15                    | 9                        | 16                      |    |                                                 |
## 3. Precision: the number of correct positive results divided by the number of positive results predicted by the classifier
![](image./precision_def.png)

Syntax: |score precision_score <actual_field_1> ... <actual_field_n> against <predicted_field_1> ... <predicted_field_n> average=<binary(default)|micro|macro|weighted> pos_label=<str|int>

...| score precision_score species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species average=micro
![](image./classifier_precision.png)

## 4. Recall: the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).
![](image./recall_def.png)

Syntax: |score recall <actual_field_1> ... <actual_field_n> against <predicted_field_1> ... <predicted_field_n> average=<binary(default)|micro|macro|weighted> pos_label=<str|int>

...| score recall species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species average=micro
![](image./classifier_recall.png)

## 5. F1 score: measure a testâ€™s accuracy, the higher the better.
![](image./f1_def.png)

Syntax: |score f1_score <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> average=<\binary(default) | micro | macro | weighted> pos_label=<\str | int>

...| score f1_score species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species average=micro
![](image./classifier_f1.png)
| n= 40                    | Actual  (Iris Setosa)                           | Actual (Iris Versicolor) | Actual (Iris Virginica) |    |                                                                         |
|--------------------------|-------------------------------------------------|--------------------------|-------------------------|----|-------------------------------------------------------------------------|
| Predict(Iris Setosa)     | 15 (TP)                                         | 0                        | 0                       | 15 | Accuracy_score = TP+TN/n = (15+9+14)/40 = 0.95                         |
| Predict(Iris Versicolor) | 0                                               | 9 (TP)                   | 2(Evir/ver)FN           | 11 | Precision= TP/TP+FP = (15+9+14)/(15+9+14)+2 = 0.95                    |
| Predict(Iris Virginica)  | 0                                               | 0                        | 14 (TP)                 | 14 |                                                                         |
|                          | 15                                              | 9                        | 16                      |    |                                                                         |
|                          | Recall= TP/TP+FN = (15+14+9)/(15+14+9)+2 = 0.95 |                          |                         |    | F1= 2*(Precison*Recall)/(Precision+Recall) = 2*(0.95*0.95)/(0.95+0.95) |


## 6.Precision-Recall-F1-Support : Compute precision, recall, F-measure and support for each class

Syntax: score precision_recall_fscore_support <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> pos_label=<\str> average=<\str> beta=<\float>

...| score precision_recall_fscore_support species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species average=micro

![](image./classifier_p_r_f.png)

... average=weighted
![](image./classifier_p_r_f_weighted.png)

## 7. ROC-AUC-score: Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. AUC is the area under the curve of plot False Positive Rate(Specificity) vs True Positive Rate(Sensitivity) at different points in [0, 1]. The higher the value, the better

Syntax: score roc_auc_score <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> pos_label=<\str | int>

	| inputlookup iris.csv
	 
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit LogisticRegression species from * probabilities=True into LR_iris

![](fit_LR_iris.png)

	| inputlookup iris.csv	 
	| sample partitions=100 seed=1234
	| search partition_number > 70 
	| apply LR_iris probabilities=True 
	| score roc_auc_score species against "probability(species=Iris Virginica)" pos_label="Iris Virginica"

![](roc_auc_LR_iris.png)

## 8. ROC : shows how the true positive rate (tpr) varies with the false positive rate (fpr), along with the corresponding probability thresholds.
Syntax: score roc_curve <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> pos_label=<\str | int>

	..| score roc_curve species against "probability(species=Iris Virginica)" pos_label="Iris Virginica"

![](roc_LR_iris.png)
