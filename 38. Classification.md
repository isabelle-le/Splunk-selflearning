# I. Classification
Classification refers to the task of giving a machine learning algorithm features, and having the algorithm put the instances/data points into one of many discrete classes. Classes are categorical in nature, it isn't possible for an instance to be classified as partially one class and partially another.
# II.Command : fit and apply
![](image./main_command.png)
## FIT example : sample + fit on iris data set with 9 classifications algrs
	| inputlookup iris.csv
	 
	| sample partitions=100 seed=1234
	| search partition_number <= 70

	| fit BernoulliNB species from * into Ber_iris
	| fit DecisionTreeClassifier species from * into DT_iris
	| fit GaussianNB species from * into Gau_iris
	| fit GradientBoostingClassifier species from * into GB_iris
	| fit LogisticRegression species from * into LR_iris
	| fit MLPClassifier species from * into MLP_iris
	| fit RandomForestClassifier species from * into RF_iris
	| fit SGDClassifier species from * into SGD_iris
	| fit SVM species from *  probabilities=True into SVM_iris
## Apply example : sample + apply +score
	| inputlookup iris.csv
	 
	| sample partitions=100 seed=1234
	| search partition_number > 70
	 
	| apply Ber_iris as Ber_species 
	| apply DT_iris as DT_species
	| apply Gau_iris as Gau_species
	| apply GB_iris as GB_species
	| apply LR_iris as LR_species
	| apply MLP_iris as MLP_species
	| apply RF_iris as RF_species
	| apply SGD_iris as SGD_species
	| apply SVM_ris as SVM_species
	| score precision_recall_fscore_support species ~ Ber_species DT_species Gau_species
	GB_species LR_species MLP_species RF_species SGD_species SVM_species average=weighted
	
![](image./classifier.png)

# III. 9 DIF Classifier algrs on Splunk

![](image./classifier_command.png)

## 1. BernoulliNB : implementation of the Naive Bayes classification algorithms for data that is distributed according to multivariate Bernoulli distributions.
What is Burnoulli (Bern) distributions? 

any event with 1 trial, 2 possible outcomes. Example: coin flip, 1true/1false question, vote for democratic/Republican

What is Multivariate Bern distributions? 

There may be multiple features but each one is assumed to be a binary-valued boolean variable.
Therefore, this class requires samples to be represented as binary-valued feature vectors. If handed any other kind of data, a Bern instance may binarize its input(depending on the binarize parameter)
![](image./BernNB1.png)
![](image./BernNB2.png)

### Example : Diabetes dataset with target var = response has binary outcome(1 or 0)
	| inputlookup diabetes.csv
	| sample partitions=700 seed=1234
	| search partition_number <= 500
	| fit BernoulliNB response from * into BernNB_diabetes_default alpha=1 binarize=0 
	fit_prior=True partial_fit=true
	| fit BernoulliNB response from * into BernNB_diabetes alpha=0.5 binarize=0 fit_prior=f


	| inputlookup diabetes.csv
	| sample partitions=700 seed=1234
	| search partition_number > 500
	| apply BernNB_diabetes_default as pred_res_default
	| apply BernNB_diabetes as pred_response
	| score accuracy_score response ~ pred_res_default pred_response 

![](image./BernNB_diabetes.png)
## 2. DecisionTreeClassifier: capable of performing multi-class classification on a dataset
Decisions tree learn from data to approximate a sine curve with a set of if-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model
![](image./DT2.png)
![](image./DT1.png)

Parameters:

1. max_depth: None(default), keep expanding until all leaves are pure

2. max_feature: nb of features to consider when looking for the best split

![](image./DT_max_feature.png)

3. min_samples_split: int or float, default=1. The minimum number of samples required to be at a leaf node.

4. max_leaf_nodes: int, default=None. Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.

5. criterion: gini(default) = gini impurity / entropy = information gain

6. splitter: best(default)= best split at each node/ random= random split

7. random_state: generate the seed used by the random number generator.

### Example :iris dataset with 3 classes in target var(species)
	| inputlookup iris.csv 
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit DecisionTreeClassifier species from * into DT_iris_default max_features=auto 
	criterion=gini splitter=best random_state=1234
	| fit DecisionTreeClassifier species from * into DT_iris max_features=2 random_state=1234

	| inputlookup iris.csv
	| sample partitions=100 seed=1234
	| search partition_number > 70
	| apply DT_iris_default as DT_species_default
	| apply DT_iris as DT_species
	| score accuracy_score species ~ DT_species_default DT_species

![](image./DT_iris.png)
|summary DT_iris_default

![](image./DT_iris_summary.png)
## 3. GaussianNB: Because of the assumption of the normal distribution, Gaussian Naive Bayes is best used in cases when all our features are continuous.
![](image./GaussianNB1.png)
![](image./GaussianNB2.png)
### Example: iris dataset
	| inputlookup iris.csv 
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit GaussianNB species from * into GauNB_iris_default partial_fit=false random_state=1234
	| fit GaussianNB species from * into GauNB_iris partial_fit=true random_state=1234

	| inputlookup iris.csv
	| sample partitions=100 seed=1234
	| search partition_number > 70
	| apply GauNB_iris_default as GauNB_species_default
	| apply GauNB_iris as GauNB_species
	| score accuracy_score species ~ GauNB_species_default GauNB_species
![](image./GaussianNB_iris.png)

## 4. GradientBoostingClassifier: Gradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to create a strong predictive model. Decision trees are usually used when doing gradient boosting.
### Adaboost:
![](image./GBclassifier1.png)
### How Gradient Boosting Works: Gradient boosting involves three elements:

	A loss function to be optimized.
	A weak learner to make predictions.
	An additive model to add weak learners to minimize the loss function.
### 1. Loss function:
	can be various. ex: regression uses squared error and classification uses logarithmic loss
### 2. Week Leaner:
	Decision trees are used as the weak learner in gradient boosting.
### 3. Additive Model:
	Trees are added one at a time, and existing trees in the model are not changed.
	A gradient descent procedure is used to minimize the loss when adding trees.
	Traditionally, gradient descent is used to minimize a set of parameters (the coefficients in a regression equation,
	 	weights in a neural network). After calculating error or loss, the weights are updated to minimize that error.
	The output for the new tree is then added to the output of the existing sequence of trees in an effort to correct or 
		improve the final output of the model.

![](image./GBclassifier2.png)
Paramater:
1.[loss=<deviance | exponential>]: loss function to be optimized.  ‘deviance’ refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss ‘exponential’ gradient boosting recovers the AdaBoost algorithm.

2.[learning_rate =<float>]: learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.Default is 0.1

3.[max_features=<str>]: same as DT

4.[min_weight_fraction_leaf=<float>]: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. Default = 0.1

5.[n_estimators=<int>] : The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Default= 100

6.[max_depth=<int>]  : same as DT

7.[min_samples_split =<int>] : same as DT

8.[min_samples_leaf=<int>] : same as DT

9.[max_leaf_nodes=<int>] : same as DT

10.[random_state=<int>] 

### Example: iris data set
	| inputlookup iris.csv 
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit GradientBoostingClassifier species from * into GB_iris_default loss=deviance learning_rate=0.1 n_estimators=100 random_state=1234
	| fit GradientBoostingClassifier species from * into GB_iris n_estimators=500 random_state=1234

	| inputlookup iris.csv
	| sample partitions=100 seed=1234
	| search partition_number > 70
	| apply GB_iris_default as GB_species_default
	| apply GB_iris as GB_species
	| score accuracy_score species ~ GB_species_default GB_species
![](image./GB_iris.png)

# IV. Evaluate classifier results
Scoring methods name:

	1. Accuracy scoring
	2. Confusion matrix
	3. F1-score
	4. Precision
	5. Precision-Recall-F1-Support
	6. Recall
	7. ROC-AUC-score
	8. ROC-curve
Syntax:| score <scoring-method-name> <actual_field_1> ... <actual_field_n> against <predicted_field_1> ... <predicted_field_n> [options]
Options: averge = choose from table below
![](image./scoring_average.png)
Look at [Scoring your model](https://docs.splunk.com/Documentation/MLApp/4.2.0/User/ScoreCommand)

## 1. Accuracy scoring : get the prediction accuracy between actual-labels and predicted-labels
![](image./accuracy_score_def.png)

![](image./accuracy_score_ex.png)

Syntax: ...|score accuracy_score <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> normalize=<True|False>

...| score accuracy_score species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species
![](image./classifier_accuracy_score.png)

Result: RF and Decision tree has a highest accuracy_score in this experiment.

## 2. Consusion_matric: a matrix as output and describes the complete performance of the model.
![](image./consusion_matrix_def.png)
	
	True Positives : The cases in which we predicted YES and the actual output was also YES.
	True Negatives : The cases in which we predicted NO and the actual output was NO.
	False Positives : The cases in which we predicted YES and the actual output was NO.
	False Negatives : The cases in which we predicted NO and the actual output was YES.

Syntax: score confusion_matrix <\actual_field> against <\predicted_field>

...| score confusion_matrix species against DT_species

![](image./consusion_matrix_ex.png)

| n= 40                    | Actual  (Iris Setosa) | Actual (Iris Versicolor) | Actual (Iris Virginica) |    |                                                 |
|--------------------------|-----------------------|--------------------------|-------------------------|----|-------------------------------------------------|
| Predict(Iris Setosa)     | 15 (TP)               | 0                        | 0                       | 15 | Accuracy_score = TP+TN/n = (15+9+14)/40 = 0.95 |
| Predict(Iris Versicolor) | 0                     | 9 (TP)                   | 2(Evir/ver)FN           | 11 |                                                 |
| Predict(Iris Virginica)  | 0                     | 0                        | 14 (TP)                 | 14 |                                                 |
|                          | 15                    | 9                        | 16                      |    |                                                 |
## 3. Precision: the number of correct positive results divided by the number of positive results predicted by the classifier
![](image./precision_def.png)

Syntax: |score precision_score <actual_field_1> ... <actual_field_n> against <predicted_field_1> ... <predicted_field_n> average=<binary(default)|micro|macro|weighted> pos_label=<str|int>

...| score precision_score species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species average=micro
![](image./classifier_precision.png)

## 4. Recall: the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).
![](image./recall_def.png)

Syntax: |score recall <actual_field_1> ... <actual_field_n> against <predicted_field_1> ... <predicted_field_n> average=<binary(default)|micro|macro|weighted> pos_label=<str|int>

...| score recall species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species average=micro
![](image./classifier_recall.png)

## 5. F1 score: measure a test’s accuracy, the higher the better.
![](image./f1_def.png)

Syntax: |score f1_score <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> average=<\binary(default) | micro | macro | weighted> pos_label=<\str | int>

...| score f1_score species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species average=micro
![](image./classifier_f1.png)
| n= 40                    | Actual  (Iris Setosa)                           | Actual (Iris Versicolor) | Actual (Iris Virginica) |    |                                                                         |
|--------------------------|-------------------------------------------------|--------------------------|-------------------------|----|-------------------------------------------------------------------------|
| Predict(Iris Setosa)     | 15 (TP)                                         | 0                        | 0                       | 15 | Accuracy_score = TP+TN/n = (15+9+14)/40 = 0.95                         |
| Predict(Iris Versicolor) | 0                                               | 9 (TP)                   | 2(Evir/ver)FN           | 11 | Precision= TP/TP+FP = (15+9+14)/(15+9+14)+2 = 0.95                    |
| Predict(Iris Virginica)  | 0                                               | 0                        | 14 (TP)                 | 14 |                                                                         |
|                          | 15                                              | 9                        | 16                      |    |                                                                         |
|                          | Recall= TP/TP+FN = (15+14+9)/(15+14+9)+2 = 0.95 |                          |                         |    | F1= 2*(Precison*Recall)/(Precision+Recall) = 2*(0.95*0.95)/(0.95+0.95) |


## 6.Precision-Recall-F1-Support : Compute precision, recall, F-measure and support for each class

Syntax: score precision_recall_fscore_support <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> pos_label=<\str> average=<\str> beta=<\float>

...| score precision_recall_fscore_support species ~ Ber_species DT_species Gau_species GB_species LR_species MLP_species RF_species SGD_species SVM_species average=micro

![](image./classifier_p_r_f.png)

... average=weighted
![](image./classifier_p_r_f_weighted.png)

## 7. ROC-AUC-score: Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. AUC is the area under the curve of plot False Positive Rate(Specificity) vs True Positive Rate(Sensitivity) at different points in [0, 1]. The higher the value, the better

Syntax: score roc_auc_score <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> pos_label=<\str | int>

	| inputlookup iris.csv
	 
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit LogisticRegression species from * probabilities=True into LR_iris

![](fit_LR_iris.png)

	| inputlookup iris.csv	 
	| sample partitions=100 seed=1234
	| search partition_number > 70 
	| apply LR_iris probabilities=True 
	| score roc_auc_score species against "probability(species=Iris Virginica)" pos_label="Iris Virginica"

![](roc_auc_LR_iris.png)

## 8. ROC : shows how the true positive rate (tpr) varies with the false positive rate (fpr), along with the corresponding probability thresholds.
Syntax: score roc_curve <\actual_field_1> ... <\actual_field_n> against <\predicted_field_1> ... <\predicted_field_n> pos_label=<\str | int>

	..| score roc_curve species against "probability(species=Iris Virginica)" pos_label="Iris Virginica"

![](roc_LR_iris.png)
