# 1. Birch (Balanced Iterative Reducing and Clustering using Hierarchies)

## What is this?
Output: clustered dataset
 	
	 	- only deals with metric variables(not categorical variables)
	 	- deals with large dataset by first generating a more compact summary that 
	 	retains as much distribution information as possible (Calles clustering feature),
	 	then clustering the summary data (instead of original dataset)

Algorithms overview:

![](image./brich1.png)


## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/Algorithms#Birch)

[Read it on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html)

[Read blog](https://towardsdatascience.com/machine-learning-birch-clustering-algorithm-clearly-explained-fb9838cbeed9)
## Syntax on Splunk
![](image./brich_syntax.png)

	Params:
	k : nb of cluters to devide the data into
	default name is cluster, use as to change it
	partial_fit: boolean. Defaut is false. If true, an existing model will be incrementally updated without having to retain on the full training data.

	Returns: cluster col with k categories

## Example : dataset  iris.csv without species(target variable)
	| inputlookup iris.csv 
	| fields - species
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit Birch petal_length petal_width sepal_length sepal_width k=3 partial_fit=F into my_brich

![](image./brich2.png)

	| inputlookup iris.csv 
	| fields - species
	| sample partitions=100 seed=1234
	| search partition_number > 70
	| apply my_brich

![](image./brich3.png)
![](image./brich4.png)

# 2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

## What is this?
DBSCAN groups together points that are close to each other based on a distance measurement (usually Euclidean distance) and a minimum number of points. It also marks as outliers the points that are in low-density regions.

DBSCAN is distinct from K-Means in that it clusters results based on local density, and uncovers a variable number of clusters, whereas K-Means finds a precise number of clusters and it needs k as an input. For example, k=5 finds 5 clusters.

Params tuning:
eps:  if the eps value chosen is too small, a large part of the data will not be clustered. It will be considered outliers because don’t satisfy the number of points to create a dense region. On the other hand, if the value that was chosen is too high, clusters will merge and the majority of objects will be in the same cluster. The eps should be chosen based on the distance of the dataset (we can use a k-distance graph to find it), but in general small eps values are preferable.

min_samples: As a general rule, a minimum minPoints can be derived from a number of dimensions (D) in the data set, as minPoints ≥ D + 1. Larger values are usually better for data sets with noise and will form more significant clusters. The minimum value for the minPoints must be 3, but the larger the data set, the larger the minPoints value that should be chosen.

## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/Algorithms#DBSCAN)

[Read it on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[Read blog](https://towardsdatascience.com/how-dbscan-works-and-why-should-i-use-it-443b4a191c80)
## Syntax on Splunk
![](image./dbscan_syntax.png)

	Params:
	1. eps:  specifies how close points should be to each other to be considered a part of a cluster. 
	It means that if the distance between two points is lower or equal to this value (eps), 
	these points are considered neighbors.

	2. min_samples: the minimum number of points to form a dense region (min=3, default=5)
	For example, if we set the minPoints parameter as 5, then we need at least 5 points to form a dense region.


	Returns: cluster col 

	Disadvantage: 
	1. No saving model
	2. Can not turning with min_samples in experiment

## Example : dataset  iris.csv without species(target variable)
	| inputlookup iris.csv 
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit DBSCAN petal_length petal_width sepal_length sepal_width eps=0.5 min_samples=35 into my_DBSCAN
	|table species cluster 
	|stats count(cluster) as predict_cluster by species

![](image./dbscan1.png)
![](image./dbscan2.png)
![](image./dbscan3.png)

# 3. K-means
## What is this?
K-means groups similar data points, with the number of groups represented by the variable k. K-means required at least approximate knowledge of the total number of groups into which the data can be divided.

Using the K-means algorithm has the following advantages:

	1. Computationally faster than most other clustering algorithms.
	2. Simple algorithm to explain and understand.
	3. Normally produces tighter clusters than hierarchical clustering.

Using the K-means algorithm has the following disadvantages:

	1. Difficult to determine optimal or true value of k. See X-means
	2. Sensitive to scaling. See StandardScaler.
	3. Each clustering may be slightly different, 
		unless you specify the random_state parameter.
	4. Does not work well with clusters of different sizes and density.

## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/Algorithms#K-means)

[Read it on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html?highlight=k%20means#sklearn.cluster.KMeans)

[Read blog]()
## Syntax on Splunk
![](image./dbscan_syntax.png)

	Params:
	1. k: pecifies the number of clusters to divide the data into

	Returns: cluster  + cluster_distance cols 

## Example : dataset  iris.csv without species(target variable)
| inputlookup iris.csv 
	| fields - species
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit KMeans petal_length petal_width sepal_length sepal_width k=3 random_state=1234 into my_kmeans 
	|stats count by cluster

![](image./kmeans1.png)
![](image./kmeans2.png)
![](image./kmeans3.png)

# 4. SpectralClustering

## What is this?
Spectral clustering first transforms the input data using the Radial Basis Function rbf kernel, and then performs K-means clustering on the result. So Spectral clustering can learn clusters with s non-convex shape.

Can not saved model so use the trick of RF. 

![](image./SC1.png)

## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/Algorithms#SpectralClustering)

[Read it on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)

[Read blog]()
## Syntax on Splunk
![](image./SC_syntax.png)

	Params:
	1. k : The dimension of the projection subspace.
	2. gamma = float. default 1.0 Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels


	Returns: cluster col with k categories

## Example : dataset  iris.csv without species(target variable)
	| inputlookup iris.csv 
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit SpectralClustering petal_length petal_width sepal_length sepal_width k=3 gamma=1.0 

![](image./SC2.png)
![](image./SC3.png)

# 5.X-means

## What is this?
X-means algr is an extented K-means that automatically determines the number of clusters based on Bayesian Information Criterion scores. 

Use X-means algr when you have no prior knowledge of the total number of labels into which that data could be divided.

Staring with a single cluster, the X-means algr goes into action after each run of K-means, making loval decisions about which subset of the current centroids should split themselves in order to fit the data better.

Using the X-means algorithm has the following advantages:

	1. Eliminates the requirement of having to provide the value of k.
	2. Normally produces tighter clusters than hierarchical clustering.

Using the X-means algorithm has the following disadvantages:

	1. Sensitive to scaling. See StandardScaler.
	2. Different initializations might result in different final clusters.
	3. Does not work well with clusters of different sizes and density.

## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/Algorithms#X-means)

[Read PDF](https://www.cs.cmu.edu/~dpelleg/download/xmeans.pdf)
## Syntax on Splunk
![](image./xmeans_syntax.png)

	Params: none

	Returns: cluster + cluster_distance cols

## Example : dataset  iris.csv without species(target variable)
Bad performance on iris dataset. n_clusters is nb of cluster, in this case is 11 (from cluster 0 to 10)
	| inputlookup iris.csv 
	| sample partitions=100 seed=1234
	| search partition_number <= 70
	| fit XMeans petal_length petal_width sepal_length sepal_width into my_xmeans
	|stats count by cluster

![](image./xmeans1.png)

	|summary my_xmeans

![](image./xmeans2.png)

Bonus: 1. Silhouette score 
       2. Pairwise distance scoring

# Bonus 1.Silhouette score
## What is this?
Silhouette score is a method of interpretation and validation of consistency within clusters of data. The silhouette value (range -1 to +1) is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). 

	+1: The objects is well matched to its own cluster and poorly matched to neighboring clusters. 
	Then the cluster configuration is appropriate
	0: The object is on or very close to the decision boundary separating two neighboring clusters.
	-1: The object have been assigned to the wrong clusters. May be too few or too many clusters

The calculation of Silhouette score can be done by using the following formula:
* silhouettescore=(p−q)/max(p,q)	

Here, p = mean distance to the points in the nearest cluster

And, q = mean intra-cluster distance to all the points.

## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/ScoreCommand#Silhouette_score)

[Read it on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)

[Read on sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)

## Syntax on Splunk
![](image./Sscore_syntax.png)

	Params: 
	metric: default is euclidean. +13 other options
	* required preprocessing ( removed NAN, only numerical data)
	predicted_array(1) vs features_array (n)

	Returns: 
	silhouette_score

## Example : dataset iris.csv without species(target variable)

	| inputlookup iris.csv
	| fit StandardScaler petal_length petal_width sepal_length sepal_width as scaled
	| fit KMeans scaled_* k=3 as KMeans_predicted_species
	| score silhouette_score KMeans_predicted_species against scaled_petal_length scaled_petal_width scaled_sepal_length scaled_sepal_width

![](image./Sscore1.png)

# Pairwise distances scoring
## What is this?
Input: a_field (single or multiple) + b_field (single or multiple)
Output: returns matrix is pairwise distance between a_field and b_field (presentable: matrix or list)

## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/ScoreCommand#Pairwise_distances_scoring)

[Read it on sklearn](https://scikit-learn.org/0.19/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html)

[Read ](http://www.cs.tau.ac.il/~rshamir/algmb/00/scribe00/html/lec08/node17.html)

## Syntax on Splunk
![](image./pw_syntax.png)

	Params: 
	metric: default is euclidean. +13 other options
	output: matrix /list
	* required preprocessing (removed NAN, only numerical data)
	array(n) against array(n)

	Returns: 
	Matrix D [n_samples_a, n_samples_b]

## Example : dataset iris.csv without species(target variable)

	| inputlookup iris.csv
	| score pairwise_distances petal_length petal_width AGAINST sepal_length sepal_width

![](image./pw1.png)
![](image./pw2.png)

* matrix cho each attributes
	| inputlookup iris.csv 
	| table petal* sepal*
	| transpose 0
![](image./pw2.png)
