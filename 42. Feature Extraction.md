# What is feature extraction vs feature selection?
Feature extraction concerning the translation of existing data into useful features

Feature selection is about creating a subset of the original pool of features.
![](image./FX1.png)

Three benefits of performing feature selection before modeling your data are:

* Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.
* Improves Accuracy: Less misleading data means modeling accuracy improves.
* Reduces Training Time: Less data means that algorithms train faster.

# 1. FieldSelector (feature selection)
## What is this?
FieldSelector uses the scikit-learn GenericUnivariateSelect.

1. Univariate feature selection works by selecting the best features based on univariate statistical tests. 

2. GenericUnivariateSelect allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator. Including:

		percentile : removes all but keep a user-specified highest scoring percentage 
		of features (default =10)
		k_best: removes all but the k highest scoring features (default=10)
		fpr: false positive rate (default=0.05)
		fdr: falsr discovery rate (default=0.05)
		fwe: family wise error (default=0.05)

		For regression: f_regression, mutual_info_regression
		For classification: chi2, f_classif, mutual_info_classif

## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/Algorithms#FieldSelector)

[Read it on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html)

## Syntax on Splunk
![](image./FS_syntax.png)

## Example : dataset dish_failures predict disk_failure(binary value)
		|inputlookup disk_failures.csv
		|fit FieldSelector type=categorical mode=k_best param=4 DiskFailure from CapacityBytes,Model,SMART_1_raw,SMART_2_raw,SMART_3_raw,SMART_4_raw,SMART_5_rawinto FS_kbest
		|fit FieldSelector type=categorical mode=percentile param=5 DiskFailure from * into FS_percentile
		|fit FieldSelector type=categorical mode=fpr DiskFailure from * into FS_fpr

Seem like a bug in this algr on Splunk. So i have to test the experiment:

* FieldSelector mode=k_best
![](image./FS_1.png)
* FieldSelector mode=percentile
![](image./FS_2.png)
* FieldSelector mode=fwe
![](image./FS_3.png)

# 2.HashingVectorizer : term frequency 
## What is this?
HashingVectorizer and CountVectorizer are meant to do the same thing. Which is to convert a collection of text documents to a matrix of token occurrences. If you need the raw counts or normalized counts (term frequency), then you should use CountVectorizer or HashingVectorizer.

If your are looking to get term frequencies weighted by their relative importance (IDF) then Tfidfvectorizer is what you should use. 
## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/Algorithms#HashingVectorizer)

[Read it on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)

[other blog](https://kavita-ganesan.com/hashingvectorizer-vs-countvectorizer/#.Xr1c5hMzbVq)
## Syntax on Splunk
![](image./HV_syntax.png)

	ngram_range:unigram or bigram
	k :n_features The number of features (columns) in the output matrices.

## Example : dataset authorization.csv
Use HashingVectorizer for term frequency and then cluster into 3 group by KMeans, giving 5 example for each cluter.

		| inputlookup authorization.csv 
		| fit HashingVectorizer Logs ngram_range=1-2 k=50 stop_words=english 
		| fit KMeans Logs_hashed* k=3 
		| fields cluster* Logs 
		| sample 5 by cluster 
		| sort by cluster
![](image./HV1.png)

# 3.ICA Independent component analysis 
## What is this?
ICA seperates a multivariate signal into additive sub-components that are maximally indepent.  The ICA model does not include a noise term for the model to be correct, meaning whitening must be applied(unlike PCA where whitenning can be done internally using whiten argument)

Whitenning(identity matrix): involves the eigen-value decomposition of its covariance matrix. 
* D : a diagonal matrix of eigenvalues
* lamda: is a eigenvalue of the covariance matrix

![](image./ICA1.png)
## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/Algorithms#ICA)

[Read it on sklearn](https://scikit-learn.org/0.19/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)

[other blog](https://towardsdatascience.com/independent-component-analysis-ica-in-python-a0ef0db0955e)

## Syntax on Splunk
![](image./ICA_syntax.png)

## Example : 

		| makeresults count=2
		| streamstats count as count
		| eval time=case(count=2,relative_time(now(),"+2d"),count=1,now())
		| makecontinuous time span=15m
		| eval _time=time
		| eval s1 = sin(2*time)
		| eval s2 = sin(4*time)
		| eval m1 = 1.5*s1 + .5*s2, m2 = .1*s1 + s2
		| fit ICA m1, m2 n_components=2 as IC
		| fit PCA m1, m2 k=2 as PC
		| fields _time, *
		| fields - count, time

![](image./ICA2.png)

The following example shows how ICA is able to find the two original sources of data from two measurements that have mixes of both. As a comparison, PCA is used to show the difference between the two â€“ PCA is not able to identify the original sources.

# 4. KernelPCA
## What is this?
PCA is a linear method (suitable to apply to datasets which are linearly separable). Kernel PCA uses a kernel function to project dataset into a higher dimensional feature space, where it is linearly separable.

Ex: nonlinearly dataset and after applied KernelPCA

![](image./non_linear.png)
![](image./KernelPCA1.png)
## Document
[Read it on Splunk](https://docs.splunk.com/Documentation/MLApp/5.1.0/User/Algorithms#KernelPCA)

[Read it on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)

[other blog on PCA kernelPCA vs ICA](https://towardsdatascience.com/kernel-pca-vs-pca-vs-ica-in-tensorflow-sklearn-60e17eb15a64)

## Syntax on Splunk
![](image./KernelPCA_syntax.png)

	degree:degree of poly kernel, default=3
	k:nb of components. default None, all non_zero components are kept
	gamma:kernel cofficient for rbf,poly,sigmoid.default 1/n_features 
	tolerance:Convergence tolerance for arpack. If 0, optimal value will be chosen by arpack.
	max_iteration:max nb of iteration(default none, choose optimal)

## Example : dataset 
	|inputlookup iris.csv
	|fit StandardScaler petal_length,petal_width,sepal_length,sepal_width as SS
	|fit KernelPCA SS_petal_length,SS_petal_width,SS_sepal_length,SS_sepal_width into kernelPCA k=3
![](image./KernelPCA2.png)
* Note: + or - means its direction so it does not mean + is better than -






















